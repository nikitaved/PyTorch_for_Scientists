{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf40f245-bfd6-4a7a-8fde-74a30afafc92",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## PyTorch for Scientists!\n",
    "by **nikitaved@github**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10e6937-5a29-4b9a-9727-4d3bb8124d2a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "\n",
    "A rough sketch of things covered:\n",
    "1. Memory <br>\n",
    "a. Topology <br>\n",
    "b. Peformance implications <br>\n",
    "c. ~Broadcasting~\n",
    "3. ~Reproducibility related to determinism and nondeterminism~\n",
    "4. ~Basics of high-level optimizations on Computational Graphs~\n",
    "5. ~Other things?~\n",
    "\n",
    "**IMPORTANT NOTE**: to, hopefully, enjoy the content even more, I recommend trying to forget anything you know and just follow the definitions. For example, if you know about contiguous tensors and their stride structure implications, forget about it before these things are introduced in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93900c08-5de7-460e-ba24-af7dd0ff4c52",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "**Python/PyTorch** versions used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33d1929b-7c4d-4eeb-962a-59c1a6cf0c5b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.13\n",
      "PyTorch version: 2.8.0.dev20250521+cu128\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "torch.manual_seed(13)\n",
    "print(f\"Python version: {'.'.join(map(str, sys.version_info[:2]))}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88026a6-b11d-4313-b980-ec597917afa9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## PyTorch in a nutshell (not exhaustive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259db739-5810-4879-bc74-c0a613f40c0c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "1. Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c1117a-abc2-48cf-9521-6ac1a64149a2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "2. Operations over Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b546e670-4fcd-4291-bd3f-57776637b98d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "3. Computational Graphs and operations with them (this includes forward/backward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0257cdcf-96db-4984-99f6-af624aa41873",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1b899f-2603-4d86-9f1e-95fe885fe4e7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "For simplicity, we are talking about the standard and very familiar, so-called, **strided** tensors (as opposed to sparse, nested and other types)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fd0e78-8099-4c9d-b9eb-3fed3de111cc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "A **strided** tensor is a N-dimensional array with the following basic methods/attributes (the list is not exhaustive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f0cc46-0c08-4757-8944-0e9ba89c87ff",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* Allocated data or memory storage (accessible via `storage()`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53661c23-226e-415d-b10c-8eb71e21ac49",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* Underlying local (i.e. device-specific) memory topology meta-data encoded in `shape` and `stride()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fcb6e7-4c28-4104-8d68-ab5c9be2151a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* Other meta-data such as `device`, `dtype` and many more..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d84831-3e60-4390-82df-fc741e5d0057",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Memory topology of a strided tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09306217-5634-49f4-9cdc-a67aff2a6a6a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Memory is linear. **How do we map a N-dimensional index to a specific memory address for individual element accesses** then?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae266c9f-8b20-4c57-a16d-31f4156110bf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "For a tensor `t`, `t.data_ptr()` returns an address in memory at which the data of `t` begins. <br>\n",
    "When we access an element $\\text{t}[i_0, \\dots, i_{N - 1}]$, \n",
    "we read `t.dtype.itemsize` bytes at address<br> $\\big(\\text{t.data\\_ptr}() + \\text{t.dtype.itemsize} \\ast \\sum_{j=0}^{N-1} i_j \\ast \\text{t.stride}()[j]\\big)$<br>\n",
    "**NOTE**: the strides are assumed to be non-negative!\n",
    "\n",
    "Examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6187a52-5f72-4dff-844b-f4690cddce21",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def memaddr_from_idx(t: torch.Tensor, idx: tuple) -> int:\n",
    "    assert len(idx) == t.ndim and isinstance(idx, tuple)\n",
    "    assert all(0 <= i < t.shape[d] for d, i in enumerate(idx))\n",
    "    mem_offset = 0\n",
    "    for d, i in enumerate(idx):\n",
    "        mem_offset += i * t.stride()[d]\n",
    "    mem_offset *= t.dtype.itemsize\n",
    "    return t.data_ptr() + mem_offset\n",
    "\n",
    "def value_from_memaddr(memaddr: int, c_type_name: str):\n",
    "    import ctypes\n",
    "    assert hasattr(ctypes, f\"c_{c_type_name}\")\n",
    "    value = getattr(ctypes, f\"c_{c_type_name}\").from_address(memaddr).value\n",
    "    return value\n",
    "\n",
    "def idx_to_value(t: torch.Tensor, idx: list, c_type_name: str):\n",
    "    memaddr = memaddr_from_idx(t, idx)\n",
    "    value = value_from_memaddr(memaddr, c_type_name)\n",
    "    return value\n",
    "    \n",
    "t = torch.rand(24, dtype=torch.float32).reshape(2, 3, 4)\n",
    "\n",
    "import itertools\n",
    "for i, idx in enumerate(itertools.product(*(range(d) for d in t.shape))):  # Generate all possible indices into t\n",
    "    assert t[*idx].item() == idx_to_value(t, idx, \"float\")  # NOTE: this is a bitwise equality!\n",
    "assert i == t.numel() - 1  # Check we looped through all the indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620424e8-5458-45c2-8868-c67d9ecfc111",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Let `t` be a Tensor, then\n",
    "\n",
    "$\n",
    "\\text{memset(t)} = \\{ \\text{t.data\\_ptr}() + k + \\text{t.dtype.itemsize} \\ast \\sum_{j=0}^{N-1} i_j \\ast \\text{t.stride}()[j] : 0 \\le k < \\text{t.dtype.itemsize}, 0 \\le i_j < \\text{t.shape}[j]\\},\\\\\n",
    "$\n",
    "\n",
    "is its **memory set** or, simply, the set of addresses (in bytes) spanning the data. Do **not overlook** dependence upon **shape** and **stride**s. It is also helpful to think of **stride**s as a \"generating set\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d218d51-210d-424b-9ae1-50f94bc92d50",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "For two tensors `t1` and `t2`, `t2` is a **view** of `t1`, if $\\text{memset(t2)} \\subseteq \\text{memset(t1)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c5c1d0-2009-4753-a041-84025480f6f9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "**View** are very important and PyTorch will try to create them whenever possible (unless copy is needed), because:\n",
    "* no **memory allocation** is needed,\n",
    "* no **kernel launch** is needed to populate that data with the exception of **in-place** operations.<br>\n",
    "  **Caveat**: only at the time of tensor creation, not necessarily tensor materialization!\n",
    "\n",
    "A **view** is trivial, if it *can* be implemented without any **kernel launch**es (**NOTE**: unless copy is a specified semantics!),<br>\n",
    "i.e. the result is just a munipulation over the meta-data like `data_ptr`, `strides`, `shape` and others which **do not read the underlying data memory**, but pointer arithmetic with `data_ptr` is fine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fbaf46-6d75-47b6-b53f-ed808564ad59",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "**Question**: when we run `t2 = op(t1)` for some PyTorch operation, how certain are we to get a **view** `t2`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c98d20-c687-4441-b172-36cd0833e186",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "**Answer**:\n",
    "* when `op` is **in-place**, we are certain that `t2` will be exactly `t1` (albeit not necessarily trivial),\n",
    "* otherwise when `op` *can* be implemented by finding approprite `data_ptr`, `shape` and `strides` (and maybe even `dtype`) for `t2` such that $\\text{memset(t2)} \\subseteq \\text{memset(t1)}$, i.e. when `op` is a trivial **view**,\n",
    "* even if `op` *can* be implemented as a trivial **view**, it does not mean it is implemented as such.<br>\n",
    "  One should **always check documentation** and **create issues if there are inconsistencies**!\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd2d67f-9834-4e81-93ad-2d519ed874ce",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### View example: accessing a single element\n",
    "\n",
    "Accessing a single element in a N-dim tensor, i.e. $\\bf{\\textbf{t}[i_1, \\dots, i_{N}]}$, which is a 0-dim tensor, is a trivial **view**. **Why?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce63d88d-dc06-4ae0-b0ed-da78fe70f1d4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Answer:\n",
    "* set `data_ptr` to the address of the element index points to,\n",
    "* set `strides` and `shape` to `()`, i.e. empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f941616-3f38-40c9-aa7b-bc66720736ed",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mem_offset(idx: tuple, strides: tuple):\n",
    "    assert isinstance(idx, tuple) and isinstance(strides, tuple)\n",
    "    assert len(idx) == len(strides)\n",
    "    mem_offset = 0\n",
    "    for i, s in zip(idx, strides):\n",
    "        mem_offset += i * s\n",
    "    return mem_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc87a57c-9843-4b7c-a8f5-2036448c46f7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y=tensor(10, device='cuda:0'), y.shape=torch.Size([]). y.stride()=()\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(24, device=\"cuda\").reshape(2, 3, 4)\n",
    "\n",
    "idx = (0, 2, 2)\n",
    "y = x[*idx]\n",
    "print(f\"{y=}, {y.shape=}. {y.stride()=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6278123c-1c61-4f28-9f13-761837ee673b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "y_data_ptr_estimate = x.data_ptr() + x.dtype.itemsize * mem_offset(idx, x.stride())\n",
    "print(y_data_ptr_estimate == y.data_ptr())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac08f03-bcc1-43ff-b97f-0450bed0871c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "**Critical takeaway**: modifying that single element in-place will alter the initial data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322e9fea-cb6a-442a-af83-e6d9209d7dcb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "**Isn't this behavior a bit weird?** Maybe, depends on the underlying semantics and how consistent it is with other indexing patterns. I can only speculate, but it seems like in this case the developers might have found it hard to justify launching a CUDA kernel for copying just a single element. A general rule of thumb: if certain things appear to be weird and inconsistent with, for example, NumPy, it could be a tradeoff decision favouring GPU performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370191ef-9614-4553-b6eb-a98eb675100e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### View example: slicing along a dimension\n",
    "Slicing along a dimension $\\bf{d}$, i.e. $\\bf{\\textbf{res} = \\textbf{t}[\\dots, \\textbf{start}_d:\\textbf{end}_d:\\textbf{step}_d, \\dots]})$, is a trivial **view**. **Why?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e234dc-1516-45de-8f61-721db3f88bf4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "`res.data_ptr` of the result should be set to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347a909f-e0df-4a53-ac3c-39ae7e85ac17",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* `res.data_ptr = t.data_ptr + t.dtype.itemsize * (start * t.strides[d])`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc76795e-da99-4053-be94-d5797f934609",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "`res.shape` should be set to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b3153b-6b95-463c-add5-44f7e563e041",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* `res.shape = t.shape[::]; res.shape[d] = len(range(start, end, step))`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb83bd4b-a1fd-4dc2-add0-3a4451f65f4c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "`res.strides` should be set to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c51e940-6716-40a1-8766-096f83047840",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* `res.strides = t.strides[::]; res.strides[d] = step * t.strides[d]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8b415a3-3e76-4617-bb4c-564be28dffae",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = torch.rand(20, 30, 40).cuda()\n",
    "slice_test = slice(2, 15, 3)  # start=2, stop=15, step=3\n",
    "slice_lenght = len(range(2, 15, 3))  # slice object does not have a lenght\n",
    "for d in range(x.ndim):  # testing our hypothesis for data_ptr, shape and strides for all dims\n",
    "    idx = [slice(0, x.shape[i]) for i in range(x.ndim)]\n",
    "    idx[d] = slice_test\n",
    "    y = x[*idx]\n",
    "    assert y.data_ptr() == x.data_ptr() + x.dtype.itemsize * (slice_test.start * x.stride()[d])\n",
    "    assert all(\n",
    "        s1 == s2 if dim != d else s1 == slice_lenght\n",
    "        for dim, (s1, s2) in enumerate(zip(y.shape, x.shape))\n",
    "    )\n",
    "    assert all(\n",
    "        s1 == s2 if dim != d else s1 == slice_test.step * x.stride()[d]\n",
    "        for dim, (s1, s2) in enumerate(zip(y.stride(), x.stride()))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ea5e48-a211-435f-a79a-01947cbb5495",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Slicing along a dim: a peculiar observation\n",
    "\n",
    "Take a look at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3990b3e9-e9c1-4f3b-b8f8-6481e50034d6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x[0, 0, 0]=tensor(10.), y[0, 0, 0]=tensor(10.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(20, 20, 20)  # NOTE: uniform from [0, 1)\n",
    "slice_dim = slice(0, 5)\n",
    "y = x[slice_dim, :, :]\n",
    "x[0, 0, 0] = 10.\n",
    "print(f\"{x[0, 0, 0]=}, {y[0, 0, 0]=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf80fb30-a696-48fe-bf4a-e9e34f715400",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "It just agrees with our trivial **view** assumptions tested above.\n",
    "\n",
    "Now take a look at:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1570300-5a52-4ee1-b765-889b631128ae",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x[0, 0, 0]=tensor(10.), y_range[0, 0, 0]=tensor(0.3641)\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(20, 20, 20)  # NOTE: uniform from [0, 1)\n",
    "range_dim = range(0, 5)\n",
    "slice_dim = slice(0, 5)\n",
    "y_range = x[range_dim, :, :]\n",
    "y_slice = x[slice_dim, :, :]\n",
    "assert torch.all(y_range == y_slice)  # the values are the same for range and slice\n",
    "x[0, 0, 0] = 10.  # NOTE: this value is definitely out of [0, 1) range\n",
    "print(f\"{x[0, 0, 0]=}, {y_range[0, 0, 0]=}\")  # NOTE: y_range is not a view of x. Modifying x did not change y_range!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b71e12-4b48-4742-8cae-216aa480fad8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "A switch from **slice** to **range** breaks semantic consitency.\n",
    "At least one of these variants is incorrect.<br>\n",
    "**Finding** such issues and **reporting** them on GitHub helps the community!\n",
    "If the issue is there, it never hurts to chime in anyway - that can speed up resolution! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8aa112-15bc-4183-82d1-2b9db6ce82a3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### View example: permutation of dimensions\n",
    "Suppose `t` is a N-dimensional tensor and $\\pi$ is a permutation of N elements.<br>\n",
    "Consider a tensor `res` defined as $\\bf{(\\textbf{res})_{i_1, \\dots, i_N} = (\\textbf{t})_{i_{\\pi(1)}, \\dots, i_{\\pi(N)}}}$, then `res` is a trivial **view** of `t`. **Why?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2ed34e-3f54-4059-998a-7e7ae6cf74e5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "`res.data_ptr` = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1601f5-36b7-4968-b2f7-0168123fdba4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* = `t.data_ptr`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414a2af1-6716-4ad6-9843-e329133f1adb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "`res.shape` = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56f482f-62e3-41d4-bab2-3f3ef93d271f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* = `[t.shape[perm[i] for i in range(N)]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6ae03a-09d3-41a4-8c6d-1b6bb9835320",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "`res.strides` = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b9a368-5f18-4c59-95ad-f626737d6891",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* = `[t.strides[perm[i] for i in range(N)]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e57d5c24-a553-4443-8b3e-2c83ee15a792",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply_perm(perm: tuple, vals: tuple) -> tuple:\n",
    "    assert len(perm) == len(vals)\n",
    "    perm_range = frozenset(perm)\n",
    "    assert len(perm_range) == len(perm) and min(perm_range) == 0 and max(perm_range) == len(perm) - 1\n",
    "    return tuple(vals[perm[i]] for i in range(len(perm)))\n",
    "                 \n",
    "x = torch.rand(2, 3, 4)\n",
    "for i, perm in enumerate(itertools.permutations(range(x.ndim))):\n",
    "    y = x.permute(perm)\n",
    "    assert y.data_ptr() == x.data_ptr()\n",
    "    assert y.shape == apply_perm(perm, x.shape)\n",
    "    assert y.stride() == apply_perm(perm, x.stride())\n",
    "assert i == (3 * 2 * 1 - 1)  # check we looped through all possible dim permutations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fa4726-9e49-4e3f-a5bc-8fd34cb41d79",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### View example: diagonal of a (square) matrix\n",
    "Suppose `t` is a tensor of shape `(n, n)`, then the 1-dimensional `res`, defined as $\\bf{(\\textbf{res})_i = (\\textbf{t})_{ii}}$, is a trivial **view**. **Why?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e12659-cce9-4da3-a524-f67618ad4897",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "`res.data_ptr` = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c423a41-a9da-4f6f-b0d9-dfb1b845f3e3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* = `t.data_ptr`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4a74b5-6310-4559-b207-67d3dda60690",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "`res.shape` is trivial. What about `res.stride`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a78ad20-1493-4cd2-98a6-4f43ea2d70dd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* When moving from $\\bf{t_{ii}}$ to $\\bf{t_{i+1, i+1}}$, we advance once along the colums with `t.strides[-1]`, and once along the rows with `t.strides[-2]`. That gives us the total offset of `t.stride[-1] + t.stride[-2]`. Since this offset is independent of $\\bf{i}$, this must be the stride value we are looking for!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d28e41e5-bc7e-4585-9b47-7c36b87c3ab9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = torch.rand(5, 5, 5)\n",
    "for perm in itertools.permutations(range(x.ndim)):  # All permutations to test all kinds of interesting stride values\n",
    "    y = x.permute(perm)[..., 0]  # Permute and then index to get a 2d tensor\n",
    "    y_diag = y.diagonal(dim1=-2, dim2=-1)\n",
    "    assert y_diag.ndim == 1\n",
    "    assert y_diag.data_ptr() == y.data_ptr()\n",
    "    assert y_diag.shape[0] == y.shape[-1]\n",
    "    assert y_diag.stride() == (y.stride()[-1] + y.stride()[-2],)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c545191-ff91-44a4-9e7c-40c8827dc423",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Contiguous tensors\n",
    "There is a special class of **strided** tensors in PyTorch, they are *canonical*, in a way. These are so-called **contiguous** tensors.<br>\n",
    "**NOTE**: most tensor factory methods, like `ones`, `zeros`, `empty`, create **contiguous** tensors by default."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ce4b4a-07b9-47c1-9640-c0a22ee60f38",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "A tensor `t` is **contiguous**, if it is empty (i.e. there are dims of size 0) or if it is 0-dimensional (a scalar) or if it is N-dimensional with N $\\ge$ 1 and\n",
    "\n",
    "$\n",
    "\\text{memset(t)} = \\text{t.data\\_ptr()} + \\{0, 1, 2, \\dots, \\text{t.dtype.itemsize} * \\text{t.numel()} \\},\n",
    "$\n",
    "\n",
    "so $\\text{memset(t)}$ does not have \"holes\" (and we also call it **contiguous**), and, additionally, `t` has the following stride structure:\n",
    "\n",
    "$\n",
    "\\begin{cases}\n",
    "\\text{t.strides}[-1] &= 1, \\\\\n",
    "\\text{t.strides}[-i - 1] &= \\text{t.shape[-i]} * \\text{t.strides[-i]}, \\text{ for } i \\in \\{1, \\dots, \\text{t.ndim}\\}.\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "**NOTE**: PyTorch only assumes non-negative strides!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f60efd7-586a-472f-9394-fd822fdbdc75",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 20, 5, 1)\n"
     ]
    }
   ],
   "source": [
    "t = torch.rand(2, 3, 4, 5)\n",
    "print(t.stride())\n",
    "assert t.is_contiguous()\n",
    "\n",
    "def check_contiguous(t):\n",
    "    if t.numel() == 0:  # Emtpy is contiguous\n",
    "        return True\n",
    "    if t.ndim == 0:  # Scalar is contiguous\n",
    "        return True\n",
    "    isc = (t.stride(-1) == 1)\n",
    "    for i in range(1, t.ndim):\n",
    "        isc = (t.stride(-i - 1) == t.shape[-i] * t.stride(-i)) and isc\n",
    "    return isc\n",
    "\n",
    "assert check_contiguous(t) == True\n",
    "assert check_contiguous(t.transpose(-1, -2)) == False\n",
    "assert check_contiguous(t.transpose(0, -1)) == False\n",
    "\n",
    "# check empty tensor\n",
    "t = torch.rand(0, 1, 0)\n",
    "assert t.is_contiguous() == check_contiguous(t)\n",
    "\n",
    "# check scalar tensor\n",
    "t = torch.rand(())\n",
    "assert t.ndim == 0 and t.is_contiguous() == check_contiguous(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5a0036-47d7-45dd-86f1-30071bd14527",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "**NOTE**: If `t` is non-empty and not a scalar, so it is of dimension N $\\ge$ 1, then `t.strides` is non-increasing as per presented definition.<br>\n",
    "This implies that if $i$ and $j$ are two N-dimensional indices such that $i < j$ lexicographically, then $\\text{t[*i]}.\\text{data\\_ptr()} < \\text{t[*j]}.\\text{data\\_ptr()}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bd9c76-a3c1-4a25-920b-d8cb7031c056",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### An immediately useful property of contiguous tensors\n",
    "Any **reshape** (or any other shape-based operation that preserves `numel`) of a **contiguous** tensor is **contiguous** and is a **view**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c73f8a02-ac5e-4733-b40e-2712331863fc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = torch.rand(24)\n",
    "y = x.reshape(12, 2)\n",
    "z = x.reshape(2, 3, 4)\n",
    "assert x.is_contiguous() == y.is_contiguous() == z.is_contiguous()\n",
    "x[0] = 10.\n",
    "assert 10. == x[0] == y[0, 0] == z[0, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5347bc1a-c180-48c8-acd3-566ea35cf2aa",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "**NOTE**: there are cases when **reshape** returns a view of a **non-contiguous** tensor. I recommend checking **torch.view** in the documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c426715-deea-447f-a67b-cb826fb899c0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### View Summary\n",
    "* **view** operations, unless **in-place**, require $\\Theta(1)$ memory and take $\\Theta(1)$ time.<br>\n",
    "  **Caveat**: this means tensor creation, not necessarily tensor materialization!\n",
    "* one can foresee memory managment techniques with allocating sufficiently large buffers and then creating **view**s from them.\n",
    "* one has to be careful with modifying tensors **in-place** - it may change the data of all other tensors they share storage with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91771673-c0cc-4736-ad0e-0f9354d974e6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Memory and performance implications\n",
    "\n",
    "Now that we are a bit familiar with the **stride**d arrays, let's see how memory and stride structure impact performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983d4a02-f7c9-4efd-934d-39a86900ccd6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "### Dimension and index hierarchy\n",
    "It is very helpful to understand how PyTorch traverses memory internally.<br>\n",
    "For example, if `t` is a N-dimensional tensor, how does PyTorch traverse `t` to, for example, compute something like `t.add_(3)`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d766938-e379-413d-b9a2-f032d9b6257c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Suppose `t` has strides $(s_1, \\dots, s_{N})$ and $\\pi$ is a dimension-sorting permutation such that\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "s_{\\pi^{-1}(1)} \\ge \\dots \\ge s_{\\pi^{-1}(N)}, \\text{ with } [s_{\\pi^{-1}(i)} = s_{\\pi^{-1}(j)}] \\implies [\\pi^{-1}(i) < \\pi^{-1}(j)],\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "which reads as *sort dimensions by stride value in decreasing order, and within groups of equal strides sort by the dimension index in ascending order*.<br>\n",
    "\n",
    "For two N-dim indices $i=(i_1, \\dots, i_N)$ and $j=(j_1, \\dots, j_N)$, let $k$ be the largest integer such that $i_{\\pi^{-1}(k)} \\neq j_{\\pi^{-1}(k)}$, then <br>\n",
    "$\n",
    "  \\begin{equation}\n",
    "    \\begin{cases}\n",
    "      i < j, & \\text{if}\\ i_{\\pi^{-1}(k)} < j_{\\pi^{-1}(k)}, \\\\\n",
    "      i > j, & \\text{if}\\ i_{\\pi^{-1}(k)} > j_{\\pi^{-1}(k)}, \\\\\n",
    "      i = j, & \\text{if no such}\\ k \\ \\text{exists}.\n",
    "    \\end{cases}\n",
    "  \\end{equation}\n",
    "$\n",
    "\n",
    "This index total order defines how PyTorch traverses memory.\n",
    "**NOTE**: if `t` is **contiguous**, then $\\pi = \\text{id}$.\n",
    "\n",
    "**In simple words**, PyTorch traverses dimensions from smallest to largest stride. This order enforces a very important property of<br>\n",
    "**Memory Locality**: $i < j \\implies \\text{t[*i]}.\\text{data\\_ptr()} \\le \\text{t[*j]}.\\text{data\\_ptr()}$ (**NOTE**: $\\le$, it becomes $<$ for **contiguous** tensors, $\\le$ is relevant in the context of *broadcasting*).<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb57de4-42bd-40d7-804c-a0f484601a2a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Why does PyTorch do that?\n",
    "\n",
    "**Memory Locality**, i.e. $i < j \\implies \\text{t[*i]}.\\text{data\\_ptr()} \\le \\text{t[*j]}.\\text{data\\_ptr()}$),<br> is **incredibly** important, because\n",
    "\n",
    "* CPU - accessing addresses close to each other reduces chances of cache misses.\n",
    "* GPU - increased chance of DRAM bursts when threads in a warp initiate coalesced memory access (contiguous memory chunks).<br>\n",
    "  Hitting the same global memory address over and over again is fine because all global memory accesses are cached.<br>\n",
    "  I recommend reading and revisiting [CUDA C Best Practices Guide](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/).<br>\n",
    "  Memory access part is [this](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/#coalesced-access-to-global-memory) one.    **Important**: pay attention to memory alignment!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f8d895-64dc-4e77-bed6-bb0a98cf90ee",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Performance implications: Unary Element-wise Operations\n",
    "**Semantics**: $\\bf{\\textbf{res}_{i_1, \\dots, i_N} = \\textbf{op}(\\text{t}_{i_1, \\dots, i_N})}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab53d86-20f1-4296-9cc6-f5ddabbc4565",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "**Question**: taking into account the semantics of unary operations and **Memory Locality**, which memory layout is likely (i.e. it is sufficient, but not necessary) to be the best for performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0fd7d9-b99c-4e09-9b1e-95ea3e7ea20d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "**Answer**: **contiguous** $\\text{memset(t)}$. This is guaranteed when, for example, the sorted array of `t.strides()` in descending order is that of a **contiguous** tensor.<br>\n",
    "**NOTE**: it could be helpful to see **contiguous** tensors as the tensors with $\\text{memset(t)}$ of the minimal *diameter* with the bijective relationship between the indices $i_1, \\dots, i_N$ and the addesses of the elements $\\text{t}[i_1, \\dots, i_N]$ with additional structure of the indices being *lexicographically* sorted. From that is also easy to derive the stride structure which we used above in the definition:\n",
    "\n",
    "$\n",
    "\\begin{cases}\n",
    "\\text{t.strides}[-i] &= 1, \\\\\n",
    "\\text{t.strides}[-i - 1] &= \\text{t.shape[-i]} * \\text{t.strides[-i]}, \\text{ for } i \\in \\{1, \\dots, \\text{t.ndim}\\}.\n",
    "\\end{cases}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8c84500-a91b-4b35-80b2-141b8557f890",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.1 ms ± 54.3 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "50.4 ms ± 703 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "77.4 ms ± 1.87 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "t_contig = torch.rand(512, 512, 512)\n",
    "t_ncontig = t_contig.permute(2, 1, 0)\n",
    "assert t_ncontig.is_contiguous() == False\n",
    "\n",
    "%timeit t_contig.add(3)\n",
    "%timeit t_ncontig.add(3)\n",
    "\n",
    "t_ncontig = torch.rand(512, 512, 512, 5)[..., 0]\n",
    "assert t_ncontig.shape == (512, 512, 512) and t_ncontig.stride()[-1] == 5\n",
    "%timeit t_ncontig.add(3)\n",
    "\n",
    "del t_contig, t_ncontig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4adff7c9-05ba-4c13-b8fe-e11a464e9dd0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5 ms ± 161 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "1.5 ms ± 120 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "4.38 ms ± 1.63 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "1.53 ms ± 279 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "def warmup(f, n_runs: int = 5):\n",
    "    \"\"\"\n",
    "    Do warmup runs of `f` assuming it does compute on the GPU.\n",
    "    Run it before profiling just to make sure that the CUDA context\n",
    "    is properly initialized.\n",
    "    \"\"\"\n",
    "    assert n_runs >= 0\n",
    "    for i in range(n_runs):\n",
    "        f()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "t_contig = torch.rand(512, 512, 512, device=\"cuda\")\n",
    "t_ncontig = t_contig.permute(2, 1, 0)\n",
    "assert t_ncontig.is_contiguous() == False\n",
    "\n",
    "warmup(lambda: t_contig.add(3))\n",
    "%timeit t_contig.add(3); torch.cuda.synchronize()\n",
    "del t_contig\n",
    "\n",
    "warmup(lambda: t_ncontig.add(3))\n",
    "%timeit t_ncontig.add(3); torch.cuda.synchronize()\n",
    "\n",
    "t_ncontig = torch.rand(512, 512, 512, 5).cuda()[..., 0]\n",
    "assert t_ncontig.shape == (512, 512, 512) and t_ncontig.stride()[-1] == 5\n",
    "warmup(lambda: t_ncontig.add(3))\n",
    "%timeit t_ncontig.add(3); torch.cuda.synchronize()\n",
    "\n",
    "# An interesting example where having \"holes\" in memset does not reduce performance by much!\n",
    "# This implies that contiguous memset is not necessary for performance!\n",
    "# Why is that? The answer is in https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/#coalesced-access-to-global-memory.\n",
    "t_ncontig = torch.rand(512, 512, 32, 512).cuda()[..., 0, :]\n",
    "assert t_ncontig.shape == (512, 512, 512) and t_ncontig.stride()[-2] == 32 * 512 and not t_ncontig.is_contiguous()\n",
    "warmup(lambda: t_ncontig.add(3))\n",
    "%timeit t_ncontig.add(3); torch.cuda.synchronize()\n",
    "\n",
    "del t_ncontig\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e99123-f831-43be-969c-ea3ef2dc5e6e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Performance implications: Reductions\n",
    "**Semantics**: $\\bf{\\textbf{res}_{i_1, \\dots, i_{d-1}, i_{d+1}, \\dots i_N} = \\textbf{op}_d(\\text{t}_{i_1, \\dots, i_{d-1}, 0, i_{d+1}, \\dots i_N},\\textbf{t}_{i_1, \\dots, i_{d-1}, 1, i_{d+1}, \\dots i_N}, \\dots, \\textbf{t}_{i_1, \\dots, i_{d-1}, \\textbf{t.shape[d]-1}, i_{d+1}, \\dots i_N})}$\n",
    "\n",
    "Examples of such operations: `max/min`, `sort`, `mean`, `var` and many others! They always have a `dim` argument."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b22d856-8e7c-4864-a2f5-d358eb247ca0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "**Question**: taking into account the semantics of unary operations and **Memory Locality**, which memory layout is the best for performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0492be6e-09d6-49e3-ac5b-43b57514383c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "**Answer**: `t.stride[d]` should be `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a49d3bf5-719b-4cd9-a40d-72a845c29c98",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48.3 μs ± 1.75 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "318 μs ± 874 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "18 μs ± 30.8 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n",
      "26.4 μs ± 15.1 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(1024, 1024)\n",
    "%timeit x.max(-1)\n",
    "%timeit x.max(-2)\n",
    "\n",
    "x = x.cuda()\n",
    "\n",
    "warmup(lambda: x.max(-1))\n",
    "%timeit x.max(-1); torch.cuda.synchronize()\n",
    "\n",
    "warmup(lambda: x.max(-2))\n",
    "%timeit x.max(-2); torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90112f4b-3437-4eb0-a06c-953b54bf8810",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Performance implications: Binary Element-wise Operations\n",
    "**Semantics**: $\\bf{\\textbf{res}_{i_1, \\dots, i_N} = \\textbf{op}(\\textbf{x}_{i_1, \\dots, i_N}, \\textbf{y}_{i_1, \\dots, i_N})}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b44c05-fa90-4df2-81a4-07d9c33977c7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "**Question**: taking into account the semantics of unary operations and **Memory Locality**, which memory layout is likely (i.e. it is sufficient, but not necessary) to be the best for performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bf1de7-cda2-469d-8668-344dfc995dbe",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "**Answer**: same as for unary operations, but, additionally, it is best for `x` and `y` to have the same strides (**Why?**)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16db41a1-72fc-4d80-b7a3-3d390547a559",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55 ms ± 115 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "205 ms ± 97.7 μs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "206 ms ± 127 μs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "55.4 ms ± 504 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(512, 512, 512)\n",
    "y = torch.rand(512, 512, 512)\n",
    "\n",
    "%timeit x * y\n",
    "%timeit x * y.transpose(0, 2)\n",
    "%timeit x.transpose(0, 2) * y\n",
    "%timeit x.transpose(0, 2) * y.transpose(0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31585639-db9b-48ca-aaf3-aab15c3142ce",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.24 ms ± 443 ns per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "19.7 ms ± 2.11 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "19.7 ms ± 3.24 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "2.25 ms ± 399 ns per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "x = x.cuda()\n",
    "y = y.cuda()\n",
    "\n",
    "f = lambda: x * y\n",
    "warmup(f)\n",
    "%timeit f(); torch.cuda.synchronize()\n",
    "\n",
    "f = lambda: x * y.transpose(0, 2)\n",
    "warmup(f)\n",
    "%timeit f(); torch.cuda.synchronize()\n",
    "\n",
    "f = lambda: x.transpose(0, 2) * y\n",
    "warmup(f)\n",
    "%timeit f(); torch.cuda.synchronize()\n",
    "\n",
    "f = lambda: x.transpose(0, 2) * y.transpose(0, 2)\n",
    "warmup(f)\n",
    "%timeit f(); torch.cuda.synchronize()\n",
    "\n",
    "del x\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6049b11-9af5-4ab2-a14b-d002481b5a1b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**NOTE**: not having the same stride structure (i.e. `x.strides() != y.strides()`) does not imply that performance is going to be significantly worse. PyTorch performs all kinds of non-trivial optimizations to yield best performance. Most of them will try to improve **Memory Locality** in the access patterns. For example, try replacing `transpose(0, 2)` with `transpose(0, 1)` or `transpose(1, 2)` and see what happens!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fe02de-b564-429b-b80e-4d6cc2cafeeb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Strides of the output\n",
    "We have seen that the stride of the inputs has performance implications. Now, suppose `res = binary_elemwise_op(x, y)`, then what is `res.stride()`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e39ef90-cb0e-40df-8e49-96897a4b35a1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "t1 = torch.rand(4, 4, 4)\n",
    "t2 = torch.rand(4, 4, 4)\n",
    "\n",
    "all_dim_perms = list(itertools.permutations(range(t1.ndim)))\n",
    "for device_type in (\"cpu\", \"cuda\"):\n",
    "    t1 = t1.to(device=device_type)\n",
    "    t2 = t2.to(device=device_type)\n",
    "    assert t1.device.type == t2.device.type == device_type\n",
    "    \n",
    "    # Let us exhaustively loop through all permutations and see what happens with the strides!\n",
    "    for perm_idx1, perm_idx2 in itertools.product(range(len(all_dim_perms)), repeat=2):\n",
    "        perm1 = all_dim_perms[perm_idx1]\n",
    "        perm2 = all_dim_perms[perm_idx2]\n",
    "    \n",
    "        x = t1.permute(perm1)\n",
    "        y = t2.permute(perm2)\n",
    "    \n",
    "        if perm1 != perm2:\n",
    "            assert x.stride() != y.stride()\n",
    "        else:\n",
    "            # because x and y are then both contiguous\n",
    "            assert x.stride() == y.stride()\n",
    "    \n",
    "        xy = x * y\n",
    "        yx = y * x\n",
    "    \n",
    "        assert torch.all(xy == yx).item()\n",
    "        assert xy.stride() == x.stride()\n",
    "        assert yx.stride() == y.stride()\n",
    "        \n",
    "assert perm_idx1 == perm_idx2 == len(all_dim_perms) - 1  # Sanity check for the last element in the Cartesian product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0d360b-5734-44d7-8df0-f36804e63a67",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "So, at least for `mul`, `res.stride() == <first argument>.stride()`. One can also see that `x * y` is not equivalent to `y * x` in terms of the strides of the output.<br>\n",
    "**Is this properly documented?** Again, one should always consult with the documentation. If it is not there, create a pull request/issue and help the community!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a67862-f2c4-4ad0-b9b1-696de16e2edf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Performance implications: Matrix Multiplication\n",
    "**Semantics**: $\\bf{\\textbf{res}_{ij} = \\sum_k \\textbf{a}_{ik} \\ast \\textbf{b}_{kj}}$. For simplicity, we assume that the arguments are 2-dimensional only!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1caaf6fe-d57a-4980-ac42-d8fb46ccbf13",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Suppose `a` is a matrix of shape `(m, n)`. Then we say it is stored in the\n",
    "* **Row-Major** (or C-contiguous) format, if $a$ is **contiguous**.\n",
    "* **Column-Major** (or F-contiguous) format, if $a^T$ is **contiguous**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d308636a-7219-4de3-a2dc-f986ac029f66",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "<img src=\"colrow_major.png\" width=\"500\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a846b361-4c1d-447f-8e44-7913436c645a",
   "metadata": {
    "editable": true,
    "raw_mimetype": "",
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "**Question**: what are the strides of a row/column-major matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c29ef5-69b2-4bef-bc13-3887ec160d05",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "**Answer**: `(n, 1)` for a row-major, and `(1, m)` for a column-major. You will need these when writing matrix operations in CUDA!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0db70e1-22e9-4e97-881d-a4bcc0b9fb8a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "**Question**: taking into account the semantics of matrix multiplications and **Memory Locality**, what are the *best* memory layouts for inputs `a` and `b` to compute their matrix product `a @ b`, if implemented *naively*?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bf4691-7095-4b4f-9f95-ea30c87c7a42",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**Answer**: `a` is row-major, `b` is column-major."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1b7e6d-9d41-404f-8ffe-bc7ee2d3c574",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "PyTorch, however, dispatches to quite fast routines that, ideally, should perform almost the same for all combination of row/column-major inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb01ce74-b9ba-47ff-9d52-822b6850b9d3",
   "metadata": {
    "editable": true,
    "raw_mimetype": "",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.5 ms ± 21.4 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "14.7 ms ± 64.2 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "16.5 ms ± 29 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "16.6 ms ± 31 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2048, 2048)\n",
    "b = torch.rand(2048, 2048)\n",
    "\n",
    "%timeit a @ b\n",
    "%timeit a @ b.mT\n",
    "%timeit a.mT @ b\n",
    "%timeit a.mT @ b.mT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2d3b9b4-1164-4af6-b77d-be3d3f0ecc5e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "833 μs ± 1.26 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "981 μs ± 1.47 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "797 μs ± 2.15 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "825 μs ± 1.55 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "a = a.cuda()\n",
    "b = b.cuda()\n",
    "\n",
    "f = lambda: a @ b\n",
    "warmup(f)\n",
    "%timeit f(); torch.cuda.synchronize()\n",
    "\n",
    "f = lambda: a @ b.mT\n",
    "warmup(f)\n",
    "%timeit f(); torch.cuda.synchronize()\n",
    "\n",
    "f = lambda: a.mT @ b\n",
    "warmup(f)\n",
    "%timeit f(); torch.cuda.synchronize()\n",
    "\n",
    "f = lambda: a.mT @ b.mT\n",
    "warmup(f)\n",
    "%timeit f(); torch.cuda.synchronize()\n",
    "\n",
    "del a, b\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aeae13b-8042-4335-99bf-7438ae308064",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Low-Rank matrix product on the GPU\n",
    "Suppose `a` is of shape `(m, k)` and `b` is of shape `(k, n)`, and `k` is relatively small.<br>\n",
    "How does `a @ b` perform with different values of `k`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce0f3946-cb88-4105-90ef-9c396ca5848b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.6 μs ± 36.1 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "38.6 μs ± 31.7 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "37.8 μs ± 50.5 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "40.8 μs ± 33.3 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "40.8 μs ± 58.7 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "def get_matrices(k):\n",
    "    x = torch.rand(2048, k, device=\"cuda\")\n",
    "    y = torch.rand(k, 2048, device=\"cuda\")\n",
    "    return x, y\n",
    "\n",
    "x, y = get_matrices(2)\n",
    "f = lambda: x @ y\n",
    "\n",
    "warmup(f)\n",
    "%timeit f(); torch.cuda.synchronize()\n",
    "\n",
    "x, y = get_matrices(4)\n",
    "warmup(f)\n",
    "%timeit f(); torch.cuda.synchronize()\n",
    "\n",
    "x, y = get_matrices(8)\n",
    "warmup(f)\n",
    "%timeit f(); torch.cuda.synchronize()\n",
    "\n",
    "x, y = get_matrices(16)\n",
    "warmup(f)\n",
    "%timeit f(); torch.cuda.synchronize()\n",
    "\n",
    "x, y = get_matrices(32)\n",
    "warmup(f)\n",
    "%timeit f(); torch.cuda.synchronize()\n",
    "\n",
    "del x, y\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac2487c-413f-4446-a547-84981dfce9ef",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**Takeaway**: PyTorch is a general-purpose library which is not optimizes for all types of shapes (especially for tall and wide matrices)! Use reasonable memory layout for matrix multiplications and use small rank operations with caution!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a6a303-aee5-4e88-8e8f-813823db5004",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Memory and Performence implications Summary\n",
    "* Stride structure of the inputs might have a huge impact on performance of operations.\n",
    "* If that is the case, sometimes making a copy with a suitable stride structure could be of benefit if you can tolerate penatly of memory allocation and copy.\n",
    "* **Contiguous** memory format, or row/column-major for matrices, is probably your best friend most of the time. If one does not deviate much from them, it is pretty hard to improve your program with direct methods (unless there is some room in the backward pass with custom `nn.Module`s) and one need to resort to some high-level advanced techniques operating on the Computational Graph itself, i.e. the so-called Deep Learning Compilers like `torch.compile` and others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72aa8cee-ef4e-4aa7-a4b2-c5e48a5b3938",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### We have just scratched the surface...\n",
    "\n",
    "PyTorch is immense, and there is always something more to tell about it.<br>\n",
    "\n",
    "If you liked what you saw here, the best thing you can do is to share this notebook with your colleagues and friends!\n",
    "\n",
    "Also, feel free to\n",
    "* create an issue on GitHub if there is an issue,\n",
    "* create an issue on GitHub if there is a topic you would like to know more about,\n",
    "* star the repository.\n",
    "\n",
    "Thank you and have fun learning High Performance Computing with PyTorch!<br>\n",
    "nikitaved@github"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
